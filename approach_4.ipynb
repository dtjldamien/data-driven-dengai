{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, AdaBoostRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "rng = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install yellowbrick with conda install -c districtdatalabs yellowbrick\n",
    "# from yellowbrick.cluster import KElbowVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of dataset, with data preprocessing\n",
    "train_features = pd.read_csv('./data/train_features_modified.csv')\n",
    "train_labels = pd.read_csv('./data/dengue_labels_train.csv')\n",
    "test_features = pd.read_csv('./data/test_features_modified.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice train_features, test_features and train_labels by city\n",
    "# Seperate data for San Juan\n",
    "sj_train_features = train_features[train_features['city'] == 'sj']\n",
    "sj_train_labels = train_labels[train_labels['city'] == 'sj']\n",
    "\n",
    "sj_test_features = test_features[test_features['city'] == 'sj']\n",
    "\n",
    "# Separate data for Iquitos\n",
    "iq_train_features = train_features[train_features['city'] == 'iq']\n",
    "iq_train_labels = train_labels[train_labels['city'] == 'iq']\n",
    "\n",
    "iq_test_features = test_features[test_features['city'] == 'iq']\n",
    "\n",
    "# drop city and week_start_date columns from train_features and test_features as they are strings\n",
    "sj_train_features.drop(['city', 'week_start_date'], axis=1, inplace=True)\n",
    "sj_test_features.drop(['city', 'week_start_date'], axis=1, inplace=True)\n",
    "\n",
    "iq_train_features.drop(['city', 'week_start_date'], axis=1, inplace=True)\n",
    "iq_test_features.drop(['city', 'week_start_date'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KElbowVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 5 clusters for our train and test data\n",
    "\n",
    "kmeans_sj = KMeans(n_clusters=5, random_state=rng).fit(sj_train_features)\n",
    "clusters_sj = kmeans_sj.predict(sj_train_features)\n",
    "clusters_sj_test = kmeans_sj.predict(sj_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_iq = KMeans(n_clusters=5, random_state=rng).fit(iq_train_features)\n",
    "clusters_iq = kmeans_iq.predict(iq_train_features)\n",
    "clusters_iq_test = kmeans_iq.predict(iq_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a cluster column in order to split the data by cluster\n",
    "\n",
    "sj_train_features['cluster'] = clusters_sj\n",
    "sj_train_labels['cluster'] = clusters_sj\n",
    "sj_test_features['cluster'] = clusters_sj_test\n",
    "\n",
    "iq_train_features['cluster'] = clusters_iq\n",
    "iq_train_labels['cluster'] = clusters_iq\n",
    "iq_test_features['cluster'] = clusters_iq_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageRegressor(BaseEstimator):\n",
    "    def __init__(self, regressors):\n",
    "        self.regressors = regressors\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for regressor in self.regressors:\n",
    "            regressor.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.array([regressor.predict(X) for regressor in self.regressors]).T\n",
    "        return np.mean(predictions, axis=1)\n",
    "\n",
    "    def score(self, X):\n",
    "        return np.mean([regressor.score(X) for regressor in self.regressors]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regressors(X, y):\n",
    "    reg1 = GradientBoostingRegressor(random_state=rng)\n",
    "    reg2 = RandomForestRegressor(random_state=rng)\n",
    "    reg3 = AdaBoostRegressor(random_state=rng)\n",
    "    \n",
    "    return [reg1, reg2, reg3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_models(train_features, train_labels):\n",
    "    best_regressors = []\n",
    "    for i in [0,1,2,3,4]:\n",
    "        train_features_cluster = train_features[train_features['cluster'] == i]\n",
    "        train_labels_cluster = train_labels[train_labels['cluster'] == i]\n",
    "        \n",
    "        regressors = get_regressors(train_features_cluster, train_labels_cluster)\n",
    "        avg_reg = AverageRegressor(regressors)\n",
    "\n",
    "        # build pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('scalar', StandardScaler(with_mean=False)),\n",
    "            ('selectkbest', SelectKBest()),\n",
    "            ('avg_regressor', avg_reg)\n",
    "        ])\n",
    "\n",
    "        # hyperparameter tuning\n",
    "        hyperparameters = {\n",
    "            'selectkbest__k': np.arange(20, 80, 10),\n",
    "            # skipped hyperparameter tuning for randomforestregressor because it requires too much compute power to run\n",
    "            # 'votingregressor__randomforestregressor__max_depth': np.arange(1, 10),\n",
    "            # 'votingregressor__randomforestregressor__n_estimators': np.arange(1, 100, 10)\n",
    "        }\n",
    "\n",
    "        estimator = GridSearchCV(pipeline, hyperparameters, cv=5)\n",
    "        estimator.fit(train_features_cluster, train_labels_cluster['total_cases'])\n",
    "\n",
    "        best_regressors.append(estimator.best_estimator_)\n",
    "    return best_regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sj_regression_models = get_regression_models(sj_train_features, sj_train_labels)\n",
    "iq_regression_models = get_regression_models(iq_train_features, iq_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sj_predictions = []\n",
    "\n",
    "# iterating each row of the test data and keeping track of the prediction for each row\n",
    "# while using the different regression models for prediction based on their cluster\n",
    "for index, row in sj_test_features.iterrows():\n",
    "    cluster = row['cluster'].astype(int)\n",
    "    pred = sj_regression_models[cluster].predict(row.values.reshape(1,-1))[0]\n",
    "    sj_predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "iq_predictions = []\n",
    "\n",
    "# iterating each row of the test data and keeping track of the prediction for each row\n",
    "# while using the different regression models for prediction based on their cluster\n",
    "for index, row in iq_test_features.iterrows():\n",
    "    cluster = row['cluster'].astype(int)\n",
    "    pred = iq_regression_models[cluster].predict(row.values.reshape(1,-1))[0]\n",
    "    iq_predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sj_predictions + iq_predictions\n",
    "\n",
    "predictions = np.array(predictions, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"./data/submission_format.csv\", index_col=[0, 1, 2])\n",
    "submission.total_cases = predictions\n",
    "submission.to_csv(\"./output/approach_4.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01fc90053a029df74eb881ecb0a44758ccef8ed513a488e06f5efd3ab0592068"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
