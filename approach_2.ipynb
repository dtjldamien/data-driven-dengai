{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc30b8b",
   "metadata": {},
   "source": [
    "# Challenge Summary\n",
    "\n",
    "Can you predict local epidemics of dengue fever?\n",
    "\n",
    "Dengue fever is a mosquito-borne disease that occurs in tropical and sub-tropical parts of the world. In mild cases, symptoms are similar to the flu: fever, rash, and muscle and joint pain. In severe cases, dengue fever can cause severe bleeding, low blood pressure, and even death.\n",
    "\n",
    "Because it is carried by mosquitoes, the transmission dynamics of dengue are related to climate variables such as temperature and precipitation. Although the relationship to climate is complex, a growing number of scientists argue that climate change is likely to produce distributional shifts that will have significant public health implications worldwide.\n",
    "\n",
    "In recent years dengue fever has been spreading. Historically, the disease has been most prevalent in Southeast Asia and the Pacific islands. These days many of the nearly half billion cases per year are occurring in Latin America:\n",
    "\n",
    "Using environmental data collected by various U.S. Federal Government agencies—from the Centers for Disease Control and Prevention to the National Oceanic and Atmospheric Administration in the U.S. Department of Commerce—can you predict the number of dengue fever cases reported each week in San Juan, Puerto Rico and Iquitos, Peru?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f53a2",
   "metadata": {},
   "source": [
    "# Team Information\n",
    "\n",
    "Name: Team Fondue\n",
    "Members:\n",
    "\n",
    "- Anthony Xavier Poh Tianci (E0406854)\n",
    "- Tan Jia Le Damien (E0310355)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c341d91",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "feb7e714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "rng = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f2c3b",
   "metadata": {},
   "source": [
    "# Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "937f6949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of dataset\n",
    "\n",
    "train_features = pd.read_csv('./dengue_features_train.csv')\n",
    "train_labels = pd.read_csv('./dengue_labels_train.csv')\n",
    "test_features = pd.read_csv('./dengue_features_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "67d9ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna\n",
    "\n",
    "train_features.fillna(method='ffill', inplace=True)\n",
    "test_features.fillna(method='ffill', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "99cd04b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert week_start_date column to datetime\n",
    "\n",
    "train_features['week_start_date'] = pd.to_datetime(\n",
    "    train_features['week_start_date'])\n",
    "test_features['week_start_date'] = pd.to_datetime(\n",
    "    test_features['week_start_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eb07ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting month to a new column\n",
    "\n",
    "train_features['month'] = train_features.week_start_date.dt.month\n",
    "test_features['month'] = test_features.week_start_date.dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9f3de27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging features and labels\n",
    "\n",
    "train_features = pd.merge(train_features, train_labels, on=[\n",
    "                          'city', 'year', 'weekofyear'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a2f70746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the average of total_cases for each week over the years\n",
    "train_features = train_features.join(train_features.groupby(['city', 'weekofyear'])[\n",
    "                                     'total_cases'].mean(), on=['city', 'weekofyear'], rsuffix='_avg')\n",
    "test_features = test_features.join(test_features.groupby(\n",
    "    ['city', 'weekofyear']).mean(), on=['city', 'weekofyear'], rsuffix='_avg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4ae539f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do rolling sum for precipitation values because precipitation builds up over time\n",
    "\n",
    "rolling_cols_sum = [\n",
    "    'precipitation_amt_mm',\n",
    "    'reanalysis_sat_precip_amt_mm',\n",
    "    'station_precip_mm'\n",
    "]\n",
    "\n",
    "# for the following columns, we take the average over a given duration\n",
    "rolling_cols_avg = [\n",
    "    'ndvi_ne',\n",
    "    'ndvi_nw',\n",
    "    'ndvi_se',\n",
    "    'ndvi_sw',\n",
    "    'reanalysis_air_temp_k',\n",
    "    'reanalysis_avg_temp_k',\n",
    "    'reanalysis_dew_point_temp_k',\n",
    "    'reanalysis_max_air_temp_k',\n",
    "    'reanalysis_min_air_temp_k',\n",
    "    'reanalysis_precip_amt_kg_per_m2',\n",
    "    'reanalysis_relative_humidity_percent',\n",
    "    'reanalysis_specific_humidity_g_per_kg',\n",
    "    'reanalysis_tdtr_k',\n",
    "    'station_avg_temp_c',\n",
    "    'station_diur_temp_rng_c',\n",
    "    'station_max_temp_c',\n",
    "    'station_min_temp_c'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8e2db4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop to create new rolling sum columns, sum over 3 weeks\n",
    "for col in rolling_cols_sum:\n",
    "    train_features['rolling_sum_' + col] = train_features[col].rolling(3).sum()\n",
    "    test_features['rolling_sum_' + col] = test_features[col].rolling(3).sum()\n",
    "\n",
    "# for loop to create new rolling average columns, mean over 3 weeks\n",
    "for col in rolling_cols_avg:\n",
    "    train_features['rolling_avg_' +\n",
    "                   col] = train_features[col].rolling(3).mean()\n",
    "    test_features['rolling_avg_' + col] = test_features[col].rolling(3).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5318cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dengue has about 4 to 10 days of incubation of dengue\n",
    "# and takes about 8 to 10 days from egg to adult, create lag of 2 weeks\n",
    "# create lag features for the following columns\n",
    "for col in rolling_cols_sum:\n",
    "    train_features['lag_1_' + col] = train_features[col].shift(1)\n",
    "    test_features['lag_1_' + col] = test_features[col].shift(1)\n",
    "    train_features['lag_2_' + col] = train_features[col].shift(2)\n",
    "    test_features['lag_2_' + col] = test_features[col].shift(2)\n",
    "\n",
    "# create lag features for the following columns\n",
    "for col in rolling_cols_avg:\n",
    "    train_features['lag_1_' + col] = train_features[col].shift(1)\n",
    "    test_features['lag_1_' + col] = test_features[col].shift(1)\n",
    "    train_features['lag_2_' + col] = train_features[col].shift(2)\n",
    "    test_features['lag_2_' + col] = test_features[col].shift(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "94bcad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use backward fill for missing values in the rolling sum and rolling average columns\n",
    "# reason for this is because our rolling sum and rolling averages take values from the previous weeks\n",
    "train_features.fillna(method='bfill', inplace=True)\n",
    "test_features.fillna(method='bfill', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46132ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "550b79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our train_features to a csv file for easier access in approach 3 and 4\n",
    "train_features.to_csv('train_features_modified.csv', mode='w', index=False)\n",
    "test_features.to_csv('test_features_modified.csv', mode='w', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ac48f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice train_features, test_features and train_labels by city\n",
    "# Seperate data for San Juan\n",
    "sj_train_features = train_features[train_features['city'] == 'sj']\n",
    "sj_train_labels = train_labels[train_labels['city'] == 'sj']\n",
    "sj_test_features = test_features[test_features['city'] == 'sj']\n",
    "\n",
    "# Separate data for Iquitos\n",
    "iq_train_features = train_features[train_features['city'] == 'iq']\n",
    "iq_train_labels = train_labels[train_labels['city'] == 'iq']\n",
    "iq_test_features = test_features[test_features['city'] == 'iq']\n",
    "\n",
    "# drop city column from train_features and test_features\n",
    "sj_train_features.drop(['city', 'week_start_date'], axis=1, inplace=True)\n",
    "sj_train_labels.drop(['city'], axis=1, inplace=True)\n",
    "sj_test_features.drop(['city', 'week_start_date'], axis=1, inplace=True)\n",
    "\n",
    "iq_train_features.drop(['city', 'week_start_date'], axis=1, inplace=True)\n",
    "iq_train_labels.drop(['city'], axis=1, inplace=True)\n",
    "iq_test_features.drop(['city', 'week_start_date'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5658a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(1, 1, self.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, self.hidden_layer_size))\n",
    "\n",
    "    # forward pass method\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = torch.from_numpy(input).float()\n",
    "        lstm_out, self.hidden_cell = self.lstm(\n",
    "            input.view(len(input), 1, -1), self.hidden_cell)\n",
    "        output = self.linear(lstm_out[-1])\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e08a7c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "# use MAE loss function since driven data uses MAE loss function to grade submissions\n",
    "loss_function = nn.L1Loss()\n",
    "# use Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "453a2cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(features, labels):\n",
    "    # train model for 10 epochs\n",
    "    epochs = 10\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(epochs):\n",
    "        # reset hidden cell state\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                             torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "        # train model\n",
    "        for i in range(len(features)):\n",
    "            model.zero_grad()\n",
    "            model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                                 torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "            # get input and output\n",
    "            inputs = features.iloc[i, :-1].values.reshape(-1, 1)\n",
    "            targets = labels.iloc[i, -1]\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model.forward(inputs)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_function(outputs, torch.tensor(targets).float())\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "        # print loss\n",
    "        print('Epoch: {}/{}, Loss: {}'.format(epoch+1, epochs, loss.item()))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1f5a2c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, features):\n",
    "    predictions = []\n",
    "    for i in range(len(features)):\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                             torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "        # get input and output\n",
    "        inputs = features.iloc[i, :-1].values.reshape(-1, 1)\n",
    "        targets = features.iloc[i, -1]\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model.forward(inputs)\n",
    "\n",
    "        # append prediction to list\n",
    "        predictions.append(outputs.item())\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "27ede405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 9.296796798706055\n",
      "Epoch: 2/10, Loss: 9.773235321044922\n",
      "Epoch: 3/10, Loss: 9.781216621398926\n",
      "Epoch: 4/10, Loss: 9.809554100036621\n",
      "Epoch: 5/10, Loss: 9.632328987121582\n",
      "Epoch: 6/10, Loss: 9.458606719970703\n",
      "Epoch: 7/10, Loss: 9.752833366394043\n",
      "Epoch: 8/10, Loss: 9.754161834716797\n",
      "Epoch: 9/10, Loss: 9.754855155944824\n",
      "Epoch: 10/10, Loss: 9.755311012268066\n",
      "Epoch: 1/10, Loss: 1.0281591415405273\n",
      "Epoch: 2/10, Loss: 1.0222787857055664\n",
      "Epoch: 3/10, Loss: 1.0364742279052734\n",
      "Epoch: 4/10, Loss: 1.0685386657714844\n",
      "Epoch: 5/10, Loss: 0.9847903251647949\n",
      "Epoch: 6/10, Loss: 1.0046987533569336\n",
      "Epoch: 7/10, Loss: 1.0044307708740234\n",
      "Epoch: 8/10, Loss: 1.0457072257995605\n",
      "Epoch: 9/10, Loss: 1.0497546195983887\n",
      "Epoch: 10/10, Loss: 0.9905633926391602\n"
     ]
    }
   ],
   "source": [
    "# train model on sj and iq data\n",
    "sj_model = train_model(sj_train_features, sj_train_labels)\n",
    "iq_model = train_model(iq_train_features, iq_train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "70fe4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sj_predictions = predict(sj_model, sj_test_features)\n",
    "iq_predictions = predict(iq_model, iq_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3bf17eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sj_predictions + iq_predictions\n",
    "# prediction as numpy of integers\n",
    "predictions = np.array(predictions, dtype=int)\n",
    "\n",
    "submission = pd.read_csv(\"./submission_format.csv\", index_col=[0, 1, 2])\n",
    "submission.total_cases = predictions\n",
    "submission.to_csv(\"./approach_2.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01fc90053a029df74eb881ecb0a44758ccef8ed513a488e06f5efd3ab0592068"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('is4242': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
