{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc30b8b",
   "metadata": {},
   "source": [
    "# Challenge Summary\n",
    "\n",
    "Can you predict local epidemics of dengue fever?\n",
    "\n",
    "Dengue fever is a mosquito-borne disease that occurs in tropical and sub-tropical parts of the world. In mild cases, symptoms are similar to the flu: fever, rash, and muscle and joint pain. In severe cases, dengue fever can cause severe bleeding, low blood pressure, and even death.\n",
    "\n",
    "Because it is carried by mosquitoes, the transmission dynamics of dengue are related to climate variables such as temperature and precipitation. Although the relationship to climate is complex, a growing number of scientists argue that climate change is likely to produce distributional shifts that will have significant public health implications worldwide.\n",
    "\n",
    "In recent years dengue fever has been spreading. Historically, the disease has been most prevalent in Southeast Asia and the Pacific islands. These days many of the nearly half billion cases per year are occurring in Latin America:\n",
    "\n",
    "Using environmental data collected by various U.S. Federal Government agencies—from the Centers for Disease Control and Prevention to the National Oceanic and Atmospheric Administration in the U.S. Department of Commerce—can you predict the number of dengue fever cases reported each week in San Juan, Puerto Rico and Iquitos, Peru?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f53a2",
   "metadata": {},
   "source": [
    "# Team Information\n",
    "\n",
    "Name: Team Fondue\n",
    "Members:\n",
    "\n",
    "- Anthony Xavier Poh Tianci (E0406854)\n",
    "- Tan Jia Le Damien (E0310355)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c341d91",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feb7e714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "rng = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f2c3b",
   "metadata": {},
   "source": [
    "# Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "937f6949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of dataset\n",
    "\n",
    "train_features = pd.read_csv('./data/dengue_features_train.csv')\n",
    "train_labels = pd.read_csv('./data/dengue_labels_train.csv')\n",
    "test_features = pd.read_csv('./data/dengue_features_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d9ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna\n",
    "train_features.fillna(method='ffill', inplace=True)\n",
    "test_features.fillna(method='ffill', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99cd04b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert week_start_date column to datetime\n",
    "\n",
    "train_features['week_start_date'] = pd.to_datetime(train_features['week_start_date'])\n",
    "test_features['week_start_date'] = pd.to_datetime(test_features['week_start_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb07ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting month to a new column\n",
    "\n",
    "train_features['month'] = train_features.week_start_date.dt.month\n",
    "test_features['month'] = test_features.week_start_date.dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f70746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1456, 25)\n",
      "(416, 25)\n"
     ]
    }
   ],
   "source": [
    "# getting the average of total_cases for each week over the years\n",
    "print(train_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae539f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do rolling sum for precipitation values because precipitation builds up over time\n",
    "\n",
    "rolling_cols_sum = [\n",
    "    'precipitation_amt_mm',\n",
    "    'reanalysis_sat_precip_amt_mm',\n",
    "    'station_precip_mm'\n",
    "]\n",
    "\n",
    "# for the following columns, we take the average over a given duration\n",
    "rolling_cols_avg = [\n",
    "    'ndvi_ne',\n",
    "    'ndvi_nw',\n",
    "    'ndvi_se',\n",
    "    'ndvi_sw',\n",
    "    'reanalysis_air_temp_k',\n",
    "    'reanalysis_avg_temp_k',\n",
    "    'reanalysis_dew_point_temp_k',\n",
    "    'reanalysis_max_air_temp_k',\n",
    "    'reanalysis_min_air_temp_k',\n",
    "    'reanalysis_precip_amt_kg_per_m2',\n",
    "    'reanalysis_relative_humidity_percent',\n",
    "    'reanalysis_specific_humidity_g_per_kg',\n",
    "    'reanalysis_tdtr_k',\n",
    "    'station_avg_temp_c',\n",
    "    'station_diur_temp_rng_c',\n",
    "    'station_max_temp_c',\n",
    "    'station_min_temp_c'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e2db4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop to create new rolling sum columns, sum over 3 weeks\n",
    "for col in rolling_cols_sum:\n",
    "    train_features['rolling_sum_' + col] = train_features[col].rolling(3).sum()\n",
    "    test_features['rolling_sum_' + col] = test_features[col].rolling(3).sum()\n",
    "\n",
    "# for loop to create new rolling average columns, mean over 3 weeks\n",
    "for col in rolling_cols_avg:\n",
    "    train_features['rolling_avg_' + col] = train_features[col].rolling(3).mean()\n",
    "    test_features['rolling_avg_' + col] = test_features[col].rolling(3).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5318cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dengue has about 4 to 10 days of incubation of dengue\n",
    "# and takes about 8 to 10 days from egg to adult, create lag of 2 weeks\n",
    "# create lag features for the following columns\n",
    "for col in rolling_cols_sum:\n",
    "    train_features['lag_1_' + col] = train_features[col].shift(1)\n",
    "    test_features['lag_1_' + col] = test_features[col].shift(1)\n",
    "    train_features['lag_2_' + col] = train_features[col].shift(2)\n",
    "    test_features['lag_2_' + col] = test_features[col].shift(2)\n",
    "\n",
    "# create lag features for the following columns\n",
    "for col in rolling_cols_avg:\n",
    "    train_features['lag_1_' + col] = train_features[col].shift(1)\n",
    "    test_features['lag_1_' + col] = test_features[col].shift(1)\n",
    "    train_features['lag_2_' + col] = train_features[col].shift(2)\n",
    "    test_features['lag_2_' + col] = test_features[col].shift(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94bcad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use backward fill for missing values in the rolling sum and rolling average columns\n",
    "# reason for this is because our rolling sum and rolling averages take values from the previous weeks\n",
    "train_features.fillna(method='bfill', inplace=True)\n",
    "test_features.fillna(method='bfill', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46132ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "550b79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our train_features to a csv file for easier access in approach 3 and 4\n",
    "train_features.to_csv('./data/train_features_modified.csv', mode='w', index=False)\n",
    "test_features.to_csv('./data/test_features_modified.csv', mode='w', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac48f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice train_features, test_features and train_labels by city\n",
    "# Seperate data for San Juan\n",
    "sj_train_features = train_features[train_features['city'] == 'sj']\n",
    "sj_train_labels = train_labels[train_labels['city'] == 'sj']['total_cases'].values.reshape(-1, 1)\n",
    "sj_test_features = test_features[test_features['city'] == 'sj']\n",
    "\n",
    "# Separate data for Iquitos\n",
    "iq_train_features = train_features[train_features['city'] == 'iq']\n",
    "iq_train_labels = train_labels[train_labels['city'] == 'iq']['total_cases'].values.reshape(-1, 1)\n",
    "iq_test_features = test_features[test_features['city'] == 'iq']\n",
    "\n",
    "# drop city column from train_features and test_features\n",
    "sj_train_features.drop(['city', 'week_start_date'], axis=1, inplace=True)\n",
    "sj_test_features.drop(['city', 'week_start_date'], axis=1, inplace=True)\n",
    "\n",
    "iq_train_features.drop(['city', 'week_start_date'], axis=1, inplace=True)\n",
    "iq_test_features.drop(['city', 'week_start_date'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f7482e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# standardize train and test features and labels\n",
    "sj_train_features = min_max_scaler.fit_transform(sj_train_features)\n",
    "sj_test_features = min_max_scaler.fit_transform(sj_test_features)\n",
    "\n",
    "iq_train_features = min_max_scaler.fit_transform(iq_train_features)\n",
    "iq_test_features = min_max_scaler.fit_transform(iq_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f23f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # hidden activation function\n",
    "        self.act = nn.ReLU()\n",
    "        # 8 hidden layer network\n",
    "        self.lin = nn.Linear(83, 512)\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.lin2 = nn.Linear(512, 256)\n",
    "        self.lin3 = nn.Linear(256, 128)\n",
    "        self.lin4 = nn.Linear(128, 64)\n",
    "        self.lin5 = nn.Linear(64, 32)\n",
    "        self.lin6 = nn.Linear(32, 16)\n",
    "        self.lin7 = nn.Linear(16, 1)\n",
    "        # dropout layer\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "\n",
    "    # forward pass mat1 mat2\n",
    "    def forward(self, xb):\n",
    "        xb = self.lin(xb)\n",
    "        xb = self.act(xb)\n",
    "        xb = self.logsigmoid(xb)\n",
    "        xb = self.act(xb)\n",
    "        xb = self.softmax(xb)\n",
    "        xb = self.act(xb)\n",
    "        xb = self.lin2(xb)\n",
    "        xb = self.act(xb)\n",
    "        xb = self.drop(xb)\n",
    "        xb = self.lin3(xb)\n",
    "        xb = self.act(xb)\n",
    "        xb = self.logsigmoid(xb)\n",
    "        xb = self.act(xb)\n",
    "        xb = self.softmax(xb)\n",
    "        xb = self.lin4(xb)\n",
    "        xb = self.drop(xb)\n",
    "        xb = self.act(xb)\n",
    "        xb = self.lin5(xb)\n",
    "        xb = self.act(xb)\n",
    "        xb = self.logsigmoid(xb)\n",
    "        xb = self.act(xb)\n",
    "        xb = self.softmax(xb)\n",
    "        xb = self.lin6(xb)\n",
    "        xb = self.drop(xb)\n",
    "        xb = self.act(xb)\n",
    "        xb = self.lin7(xb)\n",
    "        return xb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d849e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_features, train_labels):\n",
    "    # Adam optimizer\n",
    "    opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # train test split train_features and train_labels straified\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        train_features, train_labels, test_size=0.2)\n",
    "\n",
    "    # make X_train, Xvalid, y_train, y_valid tensors\n",
    "    X_train = torch.tensor(X_train).float()\n",
    "    X_valid = torch.tensor(X_valid).float()\n",
    "    y_train = torch.tensor(y_train).float()\n",
    "    y_valid = torch.tensor(y_valid).float()\n",
    "\n",
    "    train_ds = TensorDataset(X_train, y_train)\n",
    "    train_dl = DataLoader(train_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "    valid_ds = TensorDataset(X_valid, y_valid)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "    epochs = 2000\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = F.mse_loss(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*xb.size(0)\n",
    "\n",
    "        # print avg training statistics\n",
    "        train_loss = train_loss/len(train_dl)\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in valid_dl:\n",
    "                outs = model(xb)\n",
    "                valid_loss += F.mse_loss(outs, yb).item()*xb.size(0)\n",
    "\n",
    "        valid_loss = valid_loss / len(valid_dl)\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(f'Epoch {epoch}/{epochs} train loss: {train_loss:.3f} valid loss: {valid_loss:.3f}')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f4de22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(train_features, train_labels, test_features):\n",
    "    model = Net()\n",
    "    model = train_model(model, train_features, train_labels)\n",
    "    test_features_tensor = torch.tensor(test_features).float()\n",
    "    predictions = model(test_features_tensor).detach().numpy()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9060e32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2000 train loss: 111596.089 valid loss: 146187.322\n",
      "Epoch 500/2000 train loss: 79762.292 valid loss: 99113.652\n",
      "Epoch 1000/2000 train loss: 79458.717 valid loss: 99142.399\n",
      "Epoch 1500/2000 train loss: 79322.508 valid loss: 99095.723\n",
      "Epoch 0/2000 train loss: 5255.837 valid loss: 5702.096\n",
      "Epoch 500/2000 train loss: 3498.405 valid loss: 3821.148\n",
      "Epoch 1000/2000 train loss: 3496.494 valid loss: 3822.536\n",
      "Epoch 1500/2000 train loss: 3497.142 valid loss: 3819.692\n"
     ]
    }
   ],
   "source": [
    "sj_predictions = predict(sj_train_features, sj_train_labels, sj_test_features)\n",
    "iq_predictions = predict(iq_train_features, iq_train_labels, iq_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28c8365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions to 1d array and concatenate\n",
    "sj_predictions = sj_predictions.flatten()\n",
    "iq_predictions = iq_predictions.flatten()\n",
    "predictions = np.concatenate((sj_predictions, iq_predictions)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02c9d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"./data/submission_format.csv\", index_col=[0, 1, 2])\n",
    "submission.total_cases = predictions\n",
    "submission.to_csv(\"./output/approach_2.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01fc90053a029df74eb881ecb0a44758ccef8ed513a488e06f5efd3ab0592068"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('is4242': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
